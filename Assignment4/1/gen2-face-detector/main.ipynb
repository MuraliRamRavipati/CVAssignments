{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import depthai as dai\n",
    "import numpy as np\n",
    "import time\n",
    "import argparse\n",
    "\n",
    "default_nn = str((Path(__file__).parent / Path('models/face-detection-retail-0004_openvino_2021.2_4shave.blob')).resolve().absolute())\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('mobilenet_path', nargs='?', help=\"Path to mobilenet detection network blob\", default=default_nn)\n",
    "parser.add_argument('-s', '--sync', action=\"store_true\", help=\"Sync RGB output with NN output\", default=False)\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Start defining a pipeline\n",
    "pipeline = dai.Pipeline()\n",
    "\n",
    "# Define a source - color camera\n",
    "cam_rgb = pipeline.createColorCamera()\n",
    "cam_rgb.setPreviewSize(300, 300)\n",
    "cam_rgb.setInterleaved(False)\n",
    "cam_rgb.setFps(40)\n",
    "\n",
    "# Define a neural network that will make predictions based on the source frames\n",
    "detection_nn = pipeline.createMobileNetDetectionNetwork()\n",
    "detection_nn.setConfidenceThreshold(0.5)\n",
    "detection_nn.setBlobPath(args.mobilenet_path)\n",
    "detection_nn.setNumInferenceThreads(2)\n",
    "detection_nn.input.setBlocking(False)\n",
    "cam_rgb.preview.link(detection_nn.input)\n",
    "\n",
    "# Create outputs\n",
    "xout_rgb = pipeline.createXLinkOut()\n",
    "xout_rgb.setStreamName(\"rgb\")\n",
    "if(args.sync):\n",
    "    detection_nn.passthrough.link(xout_rgb.input)\n",
    "else:\n",
    "    cam_rgb.preview.link(xout_rgb.input)\n",
    "\n",
    "xout_nn = pipeline.createXLinkOut()\n",
    "xout_nn.setStreamName(\"nn\")\n",
    "detection_nn.out.link(xout_nn.input)\n",
    "\n",
    "\n",
    "# Pipeline defined, now the device is connected to\n",
    "with dai.Device(pipeline) as device:\n",
    "    # Start pipeline\n",
    "    device.startPipeline()\n",
    "\n",
    "    # Output queues will be used to get the rgb frames and nn data from the outputs defined above\n",
    "    q_rgb = device.getOutputQueue(name=\"rgb\", maxSize=4, blocking=False)\n",
    "    q_nn = device.getOutputQueue(name=\"nn\", maxSize=4, blocking=False)\n",
    "\n",
    "    start_time = time.monotonic()\n",
    "    counter = 0\n",
    "    detections = []\n",
    "    frame = None\n",
    "\n",
    "    # nn data (bounding box locations) are in <0..1> range - they need to be normalized with frame width/height\n",
    "    def frame_norm(frame, bbox):\n",
    "        norm_vals = np.full(len(bbox), frame.shape[0])\n",
    "        norm_vals[::2] = frame.shape[1]\n",
    "        return (np.clip(np.array(bbox), 0, 1) * norm_vals).astype(int)\n",
    "\n",
    "\n",
    "    while True:\n",
    "        if(args.sync):\n",
    "            # use blocking get() call to catch frame and inference result synced\n",
    "            in_rgb = q_rgb.get()\n",
    "            in_nn = q_nn.get()\n",
    "        else:\n",
    "            # instead of get (blocking) used tryGet (nonblocking) which will return the available data or None otherwise\n",
    "            in_rgb = q_rgb.tryGet()\n",
    "            in_nn = q_nn.tryGet()\n",
    "\n",
    "        if in_rgb is not None:\n",
    "            frame = in_rgb.getCvFrame()\n",
    "            cv2.putText(frame, \"NN fps: {:.2f}\".format(counter / (time.monotonic() - start_time)),\n",
    "                        (2, frame.shape[0] - 4), cv2.FONT_HERSHEY_TRIPLEX, 0.4, color=(255, 255, 255))\n",
    "\n",
    "        if in_nn is not None:\n",
    "            detections = in_nn.detections\n",
    "            counter += 1\n",
    "\n",
    "        # if the frame is available, draw bounding boxes on it and show the frame\n",
    "        if frame is not None:\n",
    "            for detection in detections:\n",
    "                bbox = frame_norm(frame, (detection.xmin, detection.ymin, detection.xmax, detection.ymax))\n",
    "                cv2.rectangle(frame, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (255, 0, 0), 2)\n",
    "\n",
    "            cv2.imshow(\"rgb\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "52e41782f98fb38972ac69e0e4a1cd20464197d6de4aa3423c902988ca4cc3bb"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('cvFall22': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
